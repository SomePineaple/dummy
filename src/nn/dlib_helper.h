//
// Created by nj60 on 11/24/25.
//

#ifndef DUMMY_DLIB_HELPER_H
#define DUMMY_DLIB_HELPER_H

#include <dlib/dnn.h>
#include <random>

namespace rummy::nn::nn_helper {
    // Mostly written by gemini
    template <typename T, typename = void>
    struct has_layer_params : std::false_type {};

    template <typename T>
    struct has_layer_params<T, std::void_t<decltype(std::declval<T>().get_layer_params())>> : std::true_type {};

    struct gaussian_initializer_visitor {
        dlib::rand& rnd;
        double mean;
        double std_dev;

        gaussian_initializer_visitor(dlib::rand& r, const double m, const double s)
            : rnd(r), mean(m), std_dev(s) {}

        template<typename T, typename U, typename E>
        void operator()(const size_t _, add_layer<T,U,E>& l) {
            // We act on the internal details of the layer (the computational part)
            auto& details = l.layer_details();

            // We use 'if constexpr' (C++17) to only compile this block for layers
            // that actually have parameters (like fc, con, affine).
            // Layers like relu or max_pool will skip this.
            if constexpr (has_layer_params<std::decay_t<decltype(details)>>::value)
            {
                // Get reference to the parameter tensor (weights and biases)
                tensor& params = details.get_layer_params();

                // Access raw data pointer
                auto* data = params.host();

                // Iterate and randomize
                for (size_t i = 0; i < params.size(); ++i)
                {
                    data[i] = (rnd.get_random_gaussian() * std_dev) + mean;
                }
            }
        }
    };

    template<typename NetType>
    void initialize_network_gaussian(NetType &net, const double mean, const double std_dev) {
        dlib::rand rnd;

        dlib::visit_layers(net, gaussian_initializer_visitor(rnd, mean, std_dev));
    }

    // Generated by chatgpt
    template <typename NetType>
    void mutate_network(NetType& net, const float mutationStrength, const float mutationChance) {
        using namespace dlib;

        std::mt19937 rng(std::random_device{}());

        std::normal_distribution dist(0.0f, mutationStrength);
        std::uniform_real_distribution chance(0.0f, 1.0f);

        // Visit all fc layers and mutate their weights + biases.
        visit_layers(net, [&](fc_<CARD_EMBEDDING_SIZE, FC_HAS_BIAS>& l) {
            auto W = l.get_weights();  // weights
            auto B = l.get_biases();   // biases

            const auto W_data = W.host();       // float*
            const auto B_data = B.host();       // float*

            const size_t W_size = W.size();
            const size_t B_size = B.size();

            for (size_t i = 0; i < W_size; ++i) {
                if (chance(rng) < mutationChance) {
                    W_data[i] += dist(rng);
                    W_data[i] = std::min(30.0f, std::max(-30.0f, W_data[i]));
                }
            }
            for (size_t i = 0; i < B_size; ++i) {
                if (chance(rng) < mutationChance) {
                    B_data[i] += dist(rng);
                    B_data[i] = std::min(30.0f, std::max(-30.0f, B_data[i]));
                }
            }
        });
    }
}

#endif //DUMMY_DLIB_HELPER_H
